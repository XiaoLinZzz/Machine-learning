{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop 9 - Introduction to Convolutional Neural Networks\n",
    "\n",
    "Code for workshop 9.  This will use Keras (within tensorflow v2) to build a small CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os, time\n",
    "import pandas as pd\n",
    "\n",
    "# Deep Learning imports\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# To plot nice figures\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "import seaborn as sns; sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the versions are OK (both should be 2 or more)\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "We will use MNIST, which is a set of small images (28x28) that contain 10 different fashion items - see below for class names and an example image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a built-in data for keras, so easily accessible\n",
    "mnist = keras.datasets.mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see how big it is\n",
    "print(X_train_full.shape)\n",
    "print(X_test.shape)\n",
    "n_total = X_train_full.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data appropriately (it starts with max of 255, but we want max of 1)\n",
    "# We will do this \"by hand\" here, but we could build a pipeline scaler for this instead\n",
    "# We also split the training set given to us into training and validation subsets\n",
    "#   The value of 5000 samples as the size of the validation set is an arbitrary choice\n",
    "X_test = X_test/255.0\n",
    "X_val, X_train = X_train_full[:5000]/255.0, X_train_full[5000:]/255.0 \n",
    "y_val, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "class_names = np.array([ \"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\" ])\n",
    "\n",
    "# Inspect some aspects of the data (in general, you should play around with the data \n",
    "#                                   more than this to get a feel for it)\n",
    "# Check that scaled types are appropriate\n",
    "print(X_train.dtype)\n",
    "print(X_val.dtype)\n",
    "# Look at first item\n",
    "print(class_names[y_train[0]])\n",
    "plt.imshow(X_train[0,:,:], cmap='gray')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at the distribution of labels in the training, validation and test sets\n",
    "plt.hist(y_train)\n",
    "plt.show()\n",
    "plt.hist(y_val)\n",
    "plt.show()\n",
    "plt.hist(y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Neural Network Code with Keras\n",
    "\n",
    "We will use the keras version built into tensorflow version 2.\n",
    "It is remarkably simple for building, training and evaluating networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some key parameters\n",
    "n_train = 3000\n",
    "n_val = 1000\n",
    "# Define the number and size of hidden layers\n",
    "hiddensizes = [16, 32, 16]\n",
    "# Define the activation function to be used by hidden layers\n",
    "actfn = \"relu\"\n",
    "# Optimiser and learning rate\n",
    "optimizer = keras.optimizers.SGD\n",
    "learningrate = 0.01   # SGD default value\n",
    "# Set size of batch and number of epochs\n",
    "batch_size = 32\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a CNN\n",
    "def model_cnn_factory(hiddensizes, actfn, optimizer, learningrate):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(filters=hiddensizes[0], kernel_size=3, strides=1, activation=actfn, padding=\"same\", \n",
    "                                  input_shape=[28, 28, 1]))    # input layer goes into this 2D convolution\n",
    "    model.add(keras.layers.MaxPooling2D(pool_size=2))          # Pool (downsize)\n",
    "    for n in hiddensizes[1:-1]:\n",
    "        model.add(keras.layers.Conv2D(filters=n, kernel_size=3, strides=1, padding=\"same\", activation=actfn))  # 2nd Conv\n",
    "        model.add(keras.layers.MaxPooling2D(pool_size=2))          # Pool (downsize)\n",
    "    model.add(keras.layers.Conv2D(filters=hiddensizes[-1], kernel_size=3, strides=1, padding=\"same\", activation=actfn))  # 2nd Conv\n",
    "    model.add(keras.layers.Flatten())                          # unravel into a 1D vector\n",
    "    model.add(keras.layers.Dense(10, activation = \"softmax\"))  # always have 10 classes\n",
    "    model.???(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer(learning_rate=learningrate), metrics=[\"accuracy\"])   \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data to be shape [Nx, Ny, 1]  (previously 2D was fine, but for CNN we need depth too)\n",
    "X_train = X_train.reshape((-1, 28, 28, 1))\n",
    "X_val = X_val.reshape((-1, 28, 28, 1))\n",
    "X_test = X_test.reshape((-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback - this is executed when fitting and will stop and restore best result\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_all(hiddensizes, actfn, optimizer, learningrate, n_train, n_val, n_epochs, batch_size):\n",
    "    model = model_cnn_factory(hiddensizes, actfn, optimizer, learningrate)\n",
    "    history = model.???(X_train[:n_train,:,:,:], y_train[:n_train], epochs=n_epochs, callbacks = [early_stopping_cb],\n",
    "                        validation_data=(X_val[:n_val,:,:,:], y_val[:n_val]))\n",
    "    max_val_acc = np.max(history.history['val_accuracy'])\n",
    "    return (max_val_acc, history, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valacc, history, model = do_all(hiddensizes, actfn, optimizer, learningrate, n_train, n_val, n_epochs, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    # Plot the results (shifting validation curves appropriately)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    n = len(history.history['accuracy'])\n",
    "    plt.plot(np.arange(0,n),history.history['accuracy'], color='orange')\n",
    "    plt.plot(np.arange(0,n),history.history['loss'],'b')\n",
    "    plt.plot(np.arange(0,n)+0.5,history.history['val_accuracy'],'r')  # offset both validation curves\n",
    "    plt.plot(np.arange(0,n)+0.5,history.history['val_loss'],'g')\n",
    "    plt.legend(['Train Acc','Train Loss','Val Acc','Val Loss'])\n",
    "    plt.grid(True)\n",
    "    plt.gca().set_ylim(0, 1) # set the vertical range to [0-1] \n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can inspect the output class predictions\n",
    "y_pred = np.argmax(model.predict(X_val[:3]),axis=0)  # use the first three val cases as an example\n",
    "print(y_pred)   # predicted classes\n",
    "print(class_names[y_pred])   # names of these classes (prediction)\n",
    "print(class_names[y_val[:3]])   # names of true classes\n",
    "# Display an image of the first val sample\n",
    "plt.imshow(X_val[0].reshape((28,28)), cmap=\"gray\")\n",
    "plt.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the model on the val set and get results (loss and accuracy both reported)\n",
    "valres = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(valres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also look at the probability of predicting each class rather than the class with max probability\n",
    "# Each row has ten probabilities (one per class)\n",
    "y_proba = model.predict(X_val[:3])\n",
    "print(y_proba.round(2))  # round to two decimal places when printing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring parameters\n",
    "\n",
    "For example, why make these particular choices for architecture and parameters.\n",
    "\n",
    "Let us systematically vary one parameter at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "res=[]\n",
    "for lr in [100, 10, 1, 0.1]:\n",
    "    print(f'Learning Rate = {lr}')\n",
    "    valacc, history, model = do_all(hiddensizes, actfn, optimizer, lr*learningrate, n_train, n_val, n_epochs, batch_size)\n",
    "    plot_history(history)\n",
    "    res += [[lr*learningrate,valacc]]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results (val accuracy vs factor that is changing)\n",
    "res=np.array(res)\n",
    "plt.plot(res[:,???],res[:,???],'-o')\n",
    "plt.xlabel('learning rate')\n",
    "plt.ylabel('val accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of layers\n",
    "res=[]\n",
    "for n in [1, 2, 3]:\n",
    "    print(f'Number of Layers = {n}')\n",
    "    valacc, history, model = do_all(hiddensizes[:n], actfn, optimizer, learningrate, n_train, n_val, n_epochs, batch_size)\n",
    "    plot_history(history)\n",
    "    res += [[n,valacc]]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results (val accuracy vs factor that is changing)\n",
    "res=np.array(res)\n",
    "plt.plot(res[:,???],res[:,???],'-o')\n",
    "plt.xlabel('number of layers')\n",
    "plt.ylabel('val accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training set size\n",
    "res=[]\n",
    "for ntr in [n_train*0.01, n_train*0.1, n_train*0.5, n_train]:\n",
    "    print(f'Training Set Size = {ntr}')\n",
    "    valacc, history, model = do_all(hiddensizes, actfn, optimizer, learningrate, int(ntr), n_val, n_epochs, batch_size)\n",
    "    plot_history(history)\n",
    "    res += [[int(ntr),valacc]]\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results (val accuracy vs factor that is changing)\n",
    "res=np.array(res)\n",
    "plt.plot(res[:,???],res[:,???],'-o')\n",
    "plt.xlabel('training set size')\n",
    "plt.ylabel('val accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
